echo "alias kn='echo Namespace: \$NS && kubectl -n \$NS'" >> ~/.bashrc
echo "alias hn='echo Namespace: \$NS && helm -n \$NS'" >> ~/.bashrc
. ~/.bashrc

git clone http://sachinkumar.shukla:glpat-RsUBCWz1nuyr18hjRyS4@61.16.69.134:20997/itcp/itcp-devops/platform-setup.git
cd platform-setup
git checkout test-automation

# -----

Commands to Run in Master node:

# Create a new user kube and add to sudoer (Run in m1, m2, m3, w1 and w2)
------------------------------------------------------------------------------------------------
# ************** Ensure to adjust /etc/resolve.conf file for nameserver... see section below how to handle that.
		sudo adduser kube
		sudo usermod -aG sudo kube



	# Switch to new user
	--------------------------------
			sudo su kube

			# Add This line in "/etc/sudoers" file.
			"kube ALL=(ALL) NOPASSWD:ALL"

			# Generate the public key and manuall add it to authorized keys(~/.ssh/authorized_keys) for passwordless access between machines
			ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N ""


			echo "export NS=default" >> ~/.bashrc
			echo "alias c='clear'" >> ~/.bashrc
			echo "alias k='kubectl'" >> ~/.bashrc
			echo "alias kn='echo Namespace: \$NS && kubectl -n \$NS'" >> ~/.bashrc
			echo "alias h='helm'" >> ~/.bashrc
			echo "alias hn='echo Namespace: \$NS && helm -n \$NS'" >> ~/.bashrc

			sudo chmod -R 755 /home/kube
			chmod 400 /home/kube/.ssh/id_rsa





			sudo apt update && sudo apt upgrade -y
			sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg lsb-release nfs-common net-tools vim
			sudo snap install helm --classic

			# Set hostname (use your respective hostname)
			sudo hostnamectl set-hostname master1 # [USER INPUT] user master2, master3, worker1 and worker2 in other machines respectively

	# Disable swap
	--------------------------------
			sudo swapoff -a
			sudo sed -i '/ swap / s/^/#/' /etc/fstab

	# Load Kernel Modules & sysctl
	--------------------------------
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
overlay
EOF

	sudo modprobe br_netfilter
	sudo modprobe overlay

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1cd
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

	sudo sysctl --system


	####
				# ----> below commands in case of conflit of ubuntu cni and kubernetes cni packages..(Remove all half installed dependencies)
		sudo dpkg --remove --force-remove-reinstreq kubelet kubeadm kubectl kubernetes-cni cnitool cnitool-plugins cri-tools
		sudo apt-get autoremove -y
		sudo apt-get clean

	####


	# Install Container Runtime
	----------------------------------------------
	sudo apt install -y containerd
	sudo mkdir -p /etc/containerd
	containerd config default | sudo tee /etc/containerd/config.toml

	# Set systemd as the cgroup driver
	sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

	sudo systemctl daemon-reload
	sudo systemctl restart containerd
	sudo systemctl enable containerd
	----------------------------------------------

	# 3 Install Kube Components

sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gpg


curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | \
  gpg --dearmor | \
  sudo tee /etc/apt/keyrings/kubernetes-apt-keyring.gpg > /dev/null

echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | \
  sudo tee /etc/apt/sources.list.d/kubernetes.list > /dev/null


sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl

sudo apt-mark hold kubelet kubeadm kubectl


# 4 Initialize Control Pane
MASTER_IP=$(ifconfig | grep inet | head -n 2 | tail -1 | cut -d ' ' -f 10)

# Initialize cluster with kubeadm (stacked etcd)

sudo kubeadm init \
  --control-plane-endpoint "172.31.44.10:6443" \
  --upload-certs \
  --pod-network-cidr=192.168.0.0/16 \
  > /home/sachin/kube.log 2>&1


 # 5 Save kubeadm join commands from above step

 #6 configure kubectl on master1 node (only)
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#7 Install CNI plugin (Calico)
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml # Only on master1

#8, 9 Issue join command from respective master and slave machines from step 4 (Ensure the different commands for each master and slave nodes)

#10 Validate cluster
kubectl get nodes -o wide
kubectl get pods -A

--------------------------------

[In case error comes and cleaning of packages required...]

sudo rm -f /etc/apt/keyrings/kubernetes-apt-keyring.gpg
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
sudo chmod 644 /etc/apt/keyrings/kubernetes-apt-keyring.gpg

echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get clean
sudo apt-get update


----------------------------------
#Pull These Images
sudo ctr -n k8s.io images pull quay.io/jetstack/cert-manager-webhook:v1.18.2
sudo ctr -n k8s.io images pull quay.io/jetstack/cert-manager-cainjector:v1.18.2
sudo ctr -n k8s.io images pull quay.io/jetstack/cert-manager-controller:v1.18.2
sudo ctr -n k8s.io images pull djkormo/adcs-issuer:2.1.0
sudo ctr -n k8s.io images pull registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
sudo ctr -n k8s.io images pull docker.io/bitnami/harbor-core:2.12.2-debian-12-r7
sudo ctr -n k8s.io images pull docker.io/bitnami/harbor-jobservice:2.12.2-debian-12-r7
sudo ctr -n k8s.io images pull docker.io/bitnami/harbor-portal:2.12.2-debian-12-r4
sudo ctr -n k8s.io images pull docker.io/bitnami/os-shell:12-debian-12-r38
sudo ctr -n k8s.io images pull docker.io/bitnami/redis:7.4.2-debian-12-r4
sudo ctr -n k8s.io images pull docker.io/bitnami/harbor-registry:2.12.2-debian-12-r7
sudo ctr -n k8s.io images pull docker.io/bitnami/harbor-registryctl:2.12.2-debian-12-r7
sudo ctr -n k8s.io images pull docker.io/bitnami/keycloak:25.0.4-debian-12-r1
sudo ctr -n k8s.io images pull docker.io/bitnami/postgresql:16.4.0-debian-12-r2
sudo ctr -n k8s.io images pull docker.io/bitnami/minio:2023.7.18-debian-11-r0
----------------------------------------------------------------------------------------------
For ADCS Issuer to work, the AD machine should be resolved to it's private IP (and not public IP)
to make sure ,the /etc/resolve.conf must contain below two entries.

nameserver 172.31.27.187
nameserver 12.0.0.2

--->
sudo vi /etc/systemd/resolved.conf
Modify Resolve section to below
	[Resolve]
	DNS=172.31.27.187
	FallbackDNS=

Restrat Service
sudo systemctl restart systemd-resolved

Change the link
sudo ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf



------------------------------[CLEAR MACHINE if PODS not working after start]---------------------------------------------
systemctl restart containerd
systemctl restart kubelet

crictl ps -a       # check containers
crictl images      # check images
crictl rmi --prune # remove unused images


systemctl stop kubelet
systemctl stop containerd

rm -rf /var/lib/containerd/*

systemctl start containerd
systemctl start kubelet

------------------------------[HARBOR]----------------------------------------------------

wget https://github.com/goharbor/harbor-helm/archive/refs/tags/v1.16.4.zip

helm repo add harbor https://helm.goharbor.io

helm -n ns-harbor install darc-harbor harbor/harbor \
  --set expose.type=nodePort \
  --set expose.nodePort.ports.http.nodePort=30080 \
  --set expose.nodePort.ports.https.nodePort=30443 \
  --set expose.nodePort.ports.notary.nodePort=30443 \
  --set expose.tls.enabled=false \
  --set externalURL="http://172.31.57.200:30080"


  http://172.31.57.200:30080/
  admin/Harbor12345

sudo docker login -u admin -p Harbor12345 http://172.31.57.200:30080

--------------------------[GITEA]-----------------------------------------------------
helm repo add bitnami https://charts.bitnami.com/bitnami
NS=itcp-mgmt
hn install configsrv bitnami/gitea
----------------------------[NIFI]--------------------------------------------
helm repo add cetic https://cetic.github.io/helm-charts
helm repo update
helm -n ns-nifi install nifi cetic/nifi --set image.tag=1.16.3
---------------------------------------------------------------------------------------

RabbitMQ, redis, geoserver, opensearch, data-prepper


webapp, sas, em, eqt,geoserver, cfgsvc




control plane ((subnet-b)
vol-002729b9a36147067


worker1 (subnet-a)



worker2 (subnet-b)




/var
/opt
/etc/kubernetes
/etc/cni/net.d



IMAGE TODO
Instance summary for i-0e126e4cd8c5039a3 (kube_ins_05) Info

assign role to machines: arn:aws:iam::008212946549:role/kube-ec2-role









. ~/.bashrc







sudo ctr -n k8s.io images list


kubectl create deployment nginx --image=nginx --replicas=5


sudo systemctl stop kubelet
sudo systemctl stop containerd



 Destroy and Restore Kubernetes Cluster
 ------------------------------------------

/etc/kubernetes/	Kubeadm config, certificates
/etc/cni/net.d/	CNI network configuration
/opt/cni/bin/	CNI binaries
/var/lib/kubelet/	Pod data, volume mounts
/var/lib/containerd/	Container images, snapshots
/var/lib/etcd/ (master)	ETCD database (only on master node)

-----------------------------------------------------------

Stop:
sudo systemctl stop kubelet
sudo systemctl stop containerd

sudo systemctl restart containerd kubelet


ip_address_master0=
ip_address_worker1=
ip_address_worker2=







I have three node kubernetes cluster with one master node and 2 worker nodes. It has been created using containerd.
I want to achieve automation where night they should be shutdown and morning ec2 machines should be started and the cluseter should be in the same state as it was before shutting down at night. Is Storing the important direcotories to S3 a good option? if not what are the alternatives?




I want to attach EBS to the each kubernetes nodes and mount below directories. how to do?
          "/etc/kubernetes"
          "/etc/cni/net.d"
          "/opt/cni/bin"
          "/var/lib/kubelet"
          "/var/lib/containerd"






master0
vol-02fca6c990fbc3bf1

worker1
vol-07ffb1abad6af7aa6


worker2
vol-0821c00b67a19e92e

#
lsblk
findmnt
sudo blkid /dev/nvme0n1p1


sudo umount /dev/nvme1n1
sudo mkfs.ext4 /dev/nvme1n1
#

sudo mkfs.ext4 /dev/nvme1n1

sudo mkdir -p /mnt/k8s


sudo mount /dev/nvme1n1 /mnt/k8s

sudo mkdir -p /mnt/k8s/etc_kubernetes
sudo mkdir -p /mnt/k8s/etc_cni_netd
sudo mkdir -p /mnt/k8s/opt_cni_bin
sudo mkdir -p /mnt/k8s/kubelet
sudo mkdir -p /mnt/k8s/containerd
sudo mkdir -p /mnt/k8s/var_lib_etcd # On Master Only




sudo mount --bind /mnt/k8s/etc_kubernetes /etc/kubernetes
sudo mount --bind /mnt/k8s/etc_cni_netd /etc/cni/net.d
sudo mount --bind /mnt/k8s/opt_cni_bin /opt/cni/bin
sudo mount --bind /mnt/k8s/kubelet /var/lib/kubelet
sudo mount --bind /mnt/k8s/containerd /var/lib/containerd
sudo mount --bind /mnt/k8s/var_lib_etcd /var/lib/etcd # On Master Only




/dev/nvme1n1 /mnt/k8s ext4 defaults,nofail,_netdev 0 2
/mnt/k8s/etc_kubernetes /etc/kubernetes none bind,nofail,_netdev 0 0
/mnt/k8s/etc_cni_netd /etc/cni/net.d none bind,nofail,_netdev 0 0
/mnt/k8s/opt_cni_bin /opt/cni/bin none bind,nofail,_netdev 0 0
/mnt/k8s/kubelet /var/lib/kubelet none bind,nofail,_netdev 0 0
/mnt/k8s/containerd /var/lib/containerd none bind,nofail,_netdev 0 0
/mnt/k8s/var_lib_etcd /var/lib/etcd none bind,nofail,_netdev 0 0 # On Master Only



k logs darc-apisix-7dd984bc6d-fg77d -n ns-apisix | grep error

curl -k -X GET https://itcp.itcotstg.ae/eventservice/ABCD/api/event-types



9 HTTP/1.1" 404 39 0.004 "-" "Go-http-client/1.1" - - - "http://darc-apisix-admin.ns-apisix.svc.cluster.local:9180"
10.184.61.18 - - [04/Aug/2025:12:02:05 +0000] darc-apisix-admin.ns-apisix.svc.cluster.local:9180 "GET /apisix/admin/upstreams/1ddb3fca HTTP/1.1" 200 600 0.003 "-" "Go-http-client/1.1" - - - "http://darc-apisix-admin.ns-apisix.svc.cluster.local:9180"
10.184.61.21 - - [04/Aug/2025:12:02:02 +0000] itcp.itcotstg.ae "GET /eventservice/ABCD/api/event-types HTTP/1.1" 504 239 60.000 "-" "curl/8.9.1" 192.168.52.80:8084 504 60.000 "http://itcp.itcotstg.ae/ABCD/api/event-types"
10.184.61.18 - - [04/Aug/2025:12:02:05 +0000] darc-apisix-admin.ns-apisix.svc.cluster.local:9180 "GET /apisix/admin/upstreams/c0114a34 HTTP/1.1" 200 476 0.003 "-" "Go-http-client/1.1" - - - "http://darc-apisix-admin.ns-apisix.svc.cluster.local:9180"
10.184.61.18 - - [04/Aug/2025:12:02:05 +0000] darc-apisix-admin.ns-apisix.svc.cluster.local:9180 "GET /apisix/admin/upstreams/21212189 HTTP/1.1" 404 39 0.003 "-" "Go-http-client/1.1" - - - "http://darc-apisix-admin.ns-apisix.svc.cluster.local:9180"
10.184.61.18 - - [04/Aug/2025:12:02:05 +0000] darc-apisix-admin.ns-apisix.svc.cluster.local:9180 "GET /apisix/admin/upstreams/1ddb3fca HTTP/1.1" 200 600 0.004 "-" "Go-http-client/1.1" - - - "http://darc-apisix-admin.ns-apisix.svc.cluster.local:9180"
10.184.61.18 - - [04/Aug/2025:12:02:05 +0000] d


POD: 192.168.249.215 ===> 192.168.52.90

One of the Node IP: 169.254.56.59:30084

em                     NodePort    10.105.12.232    <none>        8084:30084/TCP                                              117m



grep -r "ETCD_INITIAL_CLUSTER_STATE" .



kn patch pvc data-darc-apisix-etcd-1  -p '{"metadata":{"finalizers":null}}' --type=merge



kn logs -f -l app.kubernetes.io/name=apisix





















I have setup a 3 node kubernetes cluser with one master and 2 worker nodes using containerd. I want to automate the shutdown and start up process. All my nodes have attached EBS volume to backup important directories for kubernetes e.g. /etc/kubernetes, /var/lib/etcd, /opt/cni/bin and others. I follow to stop the kubelet and containerd services in both worker nodes and then after a minute in master nodes and then shutdown all 3 machines. Later when i start master node, start these services and after a minute start both the worker nodes. I find that sometime one of the worker I am not able to ssh and it doesn't connect in port 22. however other node works fine. What could go wrong here?





harbour.itcp.mrr.ste.com.... (DNS)  ====> ELB (lb-itcp-lb / apisix {previous}) ==> Target Group... (EC2)

APISIX - Kubernetes / Routes...






172.31.57.100:30080/itcp/webapp:0.2.0.10


Harbor12345

docker login 172.31.57.100:30080 -u admin

docker login internal-itcp-lb-331094515.ap-southeast-1.elb.amazonaws.com -u admin

docker pull 172.31.57.100:30080/itcp/eis-tomtom-data:0.2.0.5




itcp->uat->GiTea
GeoServer (Need data handling and automation scripts)
Nifi


https://18.140.165.20
raj.kulandai/Abcd@1234

https://18.140.165.20
sithu.htet/Bcad@1234



AWS CNAME ENTRIES

Before:
	harbor: apisix-e4b49aad2b22a87b.elb.ap-southeast-1.amazonaws.com.




Drain Worker1 (Assuming worker1 has issues)
---------------------------------------
kubectl drain worker1 --ignore-daemonsets --delete-emptydir-data

kubectl get pods -A -o wide | grep worker1 | awk '{print $1" "$2}' | while read ns pod; do
  echo "Deleting pod $pod in namespace $ns"
  kubectl delete pod $pod -n $ns --grace-period=0 --force
done


ssh to worker1
sudo systemctl stop kubelet
sudo systemctl stop containerd


sudo crictl rm -a
sudo crictl rmi -a


rm -rf /var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/*
rm -rf /var/lib/containerd/io.containerd.runtime.v2.task/*
rm -rf /var/lib/containerd/io.containerd.content.v1.content/*
rm -rf /var/lib/containerd/io.containerd.metadata.v1.bolt/meta.db




sudo systemctl start kubelet
sudo systemctl start containerd

kubectl uncordon worker1

---------------------------------------
Setup CSI

sudo apt-get update && sudo apt-get install -y nfs-common

172.31.29.29
/


kubectl create namespace csi-nfs --dry-run=client -o yaml | kubectl apply -f -

helm repo add csi-driver-nfs https://raw.githubusercontent.com/kubernetes-csi/csi-driver-nfs/master/charts
helm repo update

helm install csi-nfs csi-driver-nfs/csi-driver-nfs -n csi-nfs




---------------------------------------