# Create a new user kube and add to sudoer (Run in m1, m2, m3, w1 and w2)
------------------------------------------------------------------------------------------------
# ************** Ensure to adjust /etc/resolve.conf file for nameserver... see section below how to handle that.
		sudo adduser kube
		sudo usermod -aG sudo kube



	# Switch to new user
	--------------------------------
			sudo su kube

			# Add This line in "/etc/sudoers" file.
			"kube ALL=(ALL) NOPASSWD:ALL"

			# Generate the public key and manuall add it to authorized keys(~/.ssh/authorized_keys) for passwordless access between machines
			ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -N ""


			echo "export NS=default" >> ~/.bashrc
			echo "alias c='clear'" >> ~/.bashrc
			echo "alias k='kubectl'" >> ~/.bashrc
			echo "alias kn='echo Namespace: \$NS && kubectl -n \$NS'" >> ~/.bashrc
			echo "alias h='helm'" >> ~/.bashrc
			echo "alias hn='echo Namespace: \$NS && helm -n \$NS'" >> ~/.bashrc

			sudo chmod -R 755 /home/kube
			chmod 400 /home/kube/.ssh/id_rsa





			sudo apt update && sudo apt upgrade -y
			sudo apt install -y curl apt-transport-https ca-certificates software-properties-common gnupg lsb-release nfs-common net-tools vim
			sudo snap install helm --classic

			# Set hostname (use your respective hostname)
			sudo hostnamectl set-hostname master1 # [USER INPUT] user master2, master3, worker1 and worker2 in other machines respectively

	# Disable swap
	--------------------------------
			sudo swapoff -a
			sudo sed -i '/ swap / s/^/#/' /etc/fstab

	# Load Kernel Modules & sysctl
	--------------------------------
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
overlay
EOF

	sudo modprobe br_netfilter
	sudo modprobe overlay

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1cd
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

	sudo sysctl --system


	####
				# ----> below commands in case of conflit of ubuntu cni and kubernetes cni packages..(Remove all half installed dependencies)
		sudo dpkg --remove --force-remove-reinstreq kubelet kubeadm kubectl kubernetes-cni cnitool cnitool-plugins cri-tools
		sudo apt-get autoremove -y
		sudo apt-get clean

	####


	# Install Container Runtime
	----------------------------------------------
	sudo apt install -y containerd
	sudo mkdir -p /etc/containerd
	containerd config default | sudo tee /etc/containerd/config.toml

	# Set systemd as the cgroup driver
	sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

	sudo systemctl daemon-reload
	sudo systemctl restart containerd
	sudo systemctl enable containerd
	----------------------------------------------

	# 3 Install Kube Components

sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gpg


curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | \
  gpg --dearmor | \
  sudo tee /etc/apt/keyrings/kubernetes-apt-keyring.gpg > /dev/null

echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | \
  sudo tee /etc/apt/sources.list.d/kubernetes.list > /dev/null


sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl

sudo apt-mark hold kubelet kubeadm kubectl


# 4 Initialize Control Pane
MASTER_IP=$(ifconfig | grep inet | head -n 2 | tail -1 | cut -d ' ' -f 10)

# Initialize cluster with kubeadm (stacked etcd)

sudo kubeadm init \
  --control-plane-endpoint "172.31.44.10:6443" \
  --upload-certs \
  --pod-network-cidr=192.168.0.0/16 \
  > /home/sachin/kube.log 2>&1


 # 5 Save kubeadm join commands from above step

 #6 configure kubectl on master1 node (only)
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#7 Install CNI plugin (Calico)
kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.27.0/manifests/calico.yaml # Only on master1

#8, 9 Issue join command from respective master and slave machines from step 4 (Ensure the different commands for each master and slave nodes)

#10 Validate cluster
kubectl get nodes -o wide
kubectl get pods -A
